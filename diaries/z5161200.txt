--- IMPORTANT NOTE: most of my commits are from edzo491, rather than edzobel491 (which i signed up to the repo with) although there are some commits from the latter from PR merges/reviews.

Week 1
- Discussed with team to look at project options before lab
- Discussed this in the lab with team and decided
- We started a brainstorming doc for new features + API integrations
- Assigned people to ideas to check their feasibility
    - I looked into:
        - Sign-up/log-in with Google
        - Calendar integrations (e.g. GCal, hotmail, .ics)
        - Email integrations (e.g. GMail, SMTP)
        - Exporting results (e.g. Google Sheets, PDF)

Week 2
- Next lab we discussed what we found, cut down some and checked them with mentor
- We outlined the different sections for the proposal and assigned people to each section
- My sections were:
    - Finalise the API integrations and new features
    - Software Architecture
- On Sunday, we had a check-in on our assigned parts and I presented the finalised features + integrations
  as well as a rough plan for the technologies for the backend and frontend

Week 3
- In the lab we showed our mentor what we'd done so far and received some good feedback,
  as well as clarifying how to outline the JIRA tickets
- Throughout the week the team provided updates on storyboards and user stories
    - I reviewed these and provided comments/suggestions/questions
    - Some of these included recommending our user stories to focus more on functionality
      rather than how this would be accomplished in the UI, as that was the job of the storyboards,
      and on element and UX choices in the storyboards
- At the same time, I was working on the software architecture section, looking deeper
  into what specific packages would be required and deciding between them based on what
  I thought would best suit our needs, so that we could have a smooth start thanks to the
  clear picture of what we'd be using.
- After doing the diagram in a Google Drawing, Teresa helped to recreate this in Adobe XD
  to produce a higher fidelity version
- I then reviewed the entire report to ensure the sections were cohesive

Week 4
- I set up the backend adding Flask and all required packages and set up the dataqbase. While doing so I also noted down the instructions for others to use to setup.
- I planned out a database schema for the project
- I then implemented sign in and sign up, creating User objects in the process
- Reviewed Annisa's PRs for search

Week 5
- I realised Flask and the supporting packages were going to be too restrictive and slow, so I migrated everything to FastAPI
- I then rewrote sign in, sign up, and created logout using manual authentication via a cookie
- I also added the endpoint to create a listing
- Reviewed Annisa's PRs for auction details
- Demo'd what we've done so far :)
- Added various filters to the search endpoint
- Reviewed Annia's PRs for starring and registering
- We had our first retro

Week 6
- Created the landmarks infrastructure and integrated the Google Places API to find nearby landmarks
- Updated the listing creation endpoints to accept more listing data
- Reviewed Annisa's PRs for placing a bid and returning the bidding history

Week 7
- Added information to show:
    - the highest bid
    - whether a bid met the reserve price
- Reviewed Annisa's PRs to delete a listing and show whether a user was a RAB
- Added a limit, default sort, continuation and 'include closed auctions' to the search endpoint
- Made a plan for how we'd structure the user endpoints
- Begun work on recommendations, first publishing the endpoint returning dummy data while creating the model
- I tried out various algorithms/approaches for the machine learning model, including vectorization + euclidean distance, and ended up going for the KNN algorithm.
- Reviewed Annisa's PRs for images and basic user profile endpoints

Week 8
- I continued to work on recommendations and had made some good headway
- I reviewed Annisa's PRs for mroe user profile endpoints
- I got the recommendations model to a workable state and was able to demo it from the backend
- I continued working on recommendations, getting it to a state where it was properly integrated
- I made a few more polishes to recommendations and then merged it in
- We had our 2nd retro

Week 9
- I worked on email notifications for different auction states
    - I opted for an asynchronous scheduled task which runs every 5 seconds to check the database and then perform the appropriate auction
    - I setup an account to send the emails
    - I used SMTP to send the emails and had troubles parsing the HTML templates and sending images in the email
    - After fixing an issue with content identifiers, I finally had the emails working
- I helped out with Google SSO, providing the logic for how we could integrate this into our app
- Reviewed Annisa's PRs
- I modified teresa's script to fetch test data and fecthed some listings to bootstrap our app
- I tested the recommendations with this larger data set and made various tweaks to improve the recommendations
- I tested the product and added tickets for bugs and polish items

Week 10
- Did a dump of my database to act as the base set of test data that we'll submit
- However the file was too large, so I researched multiple ways to reduce it including supplying '-Z9' and/or '-Fc' to `pg_dump` and gzip
- Doing so, I was able to reduce the file size from 768Mb to 368Mb
- I tested zipping the repo but the result was still too large
- I went for the git lfs option and managed to get it working after struggling for a while
    

